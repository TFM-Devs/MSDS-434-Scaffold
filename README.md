![Capture](https://user-images.githubusercontent.com/67444022/119213552-c5a79900-ba74-11eb-87e8-c103d88df00e.PNG)
# MSDS-434-Scaffold
### Predicting Points Scored in an NCAA Basketball Game

The goal of my project was to see if I could construct a machine learning model that predicts points scored in an NCAA basketball game using cloud data engineering principles and serverless technologies. These principles and technologies included continuous deployment/continuous integration, cloud database solutions, and cloud-based machine learning tools. Generally, this was a data engineering exercise, meaning that the intent was to see if I could construct and link the necessary components needed for a functioning model, data pipeline, and development environment. Specifically, for this project, I used the Google Cloud Services suite of tools. Additionally, the goal was to create a simple functioning application that could act as a quality learning exercise while not getting distracted by adding complication that would take away from the learning process.

To explain the construction of the application I will discuss components in the following order. First, I will introduce the google cloud shell and virtual python environments I used to build, run, and store most of the code that represents the logic of the application. Second, I will explain how this shell environment integrates with GitHub to pull and push code updates sustainably by using different code streams. Third, I will discuss some specifics behind the application code I used to pull information out of a Google BigQuery public dataset, modify it using python and pandas, and load it back into a Google BigQuery table. Last, I will discuss the Google AutoML Tables model I created to generate predictions from the dataset using Google’s cloud computing power instead of my own local resources. Obviously, there are a lot of moving parts in any cloud application. Hopefully, I will hit the high points so that the intent and core mechanics of the application are clear to those new to Google Cloud Services or those new to cloud computing generally.

I will begin by saying that I found the Google Cloud terminal embedded within the cloud environment an efficient way to interact with the serverless virtual machine that underlies my Google Cloud Services project. This allowed me to create new files and run services such as BigQuery, GitHub, and AutoML directly from the terminal. It also saved time by reducing the number of clicks necessary to initiate or complete a process. Also, the terminal enabled me to create a python virtual environment where I could quickly write and unit test code; to effectively debug issues as they arise. Lastly, I was able to use a scaffolding process that included a requirements text file and a makefile to ensure that my python interpreter installed properly with the packages necessary to develop and execute my code. 

A key point of integration with the Google Cloud terminal is the ability to link it with my GitHub repository. I used GitHub for a few reasons. First, with GitHub, I could source control my code and create an external repository to back up my work. Second, I wanted to be able to deliver code to a test repository, run some checks against it, and then merge the test branch back into the main branch when I was ready to integrate that code into the application. Developing a new application includes a lot of experimentation and trial and error. This is normal and healthy, as long as code experimentation does not overwrite or invalidate functioning code that already exists. Luckily, the terminal can call GitHub directly after some quick (once you know the process) SSH key generation steps. From there, you can clone your repository, check out the test branch, push your code to it, and then merge back with the main branch when you are ready to integrate the code fully into the application.

The next relevant topic is to discuss some of the application code I wrote and the associated cloud technologies I used to run it. As described briefly in the introduction, I used a Google BigQuery public dataset as the ultimate source of information for my application. Specifically, the dataset I chose was the NCAA basketball dataset. This included most of the key measures and dimensions associated with all men’s college basketball games going back close to twenty years. 

There was a wide variety of information available in the NCAA data set, so the first natural question to answer is what information is relevant. I knew I wanted to predict points scored so I decided to pull a series of basic dimensions and measures typically associated scoring points in a basketball game. For example, home team name, away team name, were two key dimensions. Key measures were field goal percentage for each team as well as assists, rebounds, steals, blocked shots, etc. for each team. I was interested in understanding if something like assists was more or less important than rebounding for indicating point scoring. Essentially, this step comprised a feature selection process that could have been done a few ways. One way I considered was to apply principal component analysis (PCA) on all dimensions and measures in the dataset. However, PCA is a modeling concept more than a data engineering concept, so I decided to keep it simple and just pick a series of fields that I know to be relevant from playing and watching basketball since I was six years old.

Once I decided what features made sense to include in the model, I created a python script that connected to the BigQuery Public Dataset. Once connected, I used python and pandas to create a data frame and reduce the data down to only fields I deemed relevant. Next, I ran a process to write the reduced data frame back to the cloud in the form of a BigQuery table. This flow started out as a manual script that I could execute successfully using the terminal. However, I wanted it to be automated. 

To automate the process, I converted my python script into a Google Cloud Function. To automatically trigger this function, I created a pub/sub object using the Google Cloud messaging system. The trigger for the pub/sub message was a Google Cloud Scheduler job configured to run every morning at 8am. The entire flow operates as follows: when the cloud scheduler job runs each morning, it creates and sends the pub/sub message which triggers the cloud function. The cloud function runs the script to connect to the BigQuery public dataset, apply the python/pandas logic, and then load the result into the BigQuery table. This type of automation allows me to exclude myself from the process so that it can run on its own.

The BigQuery table resulting from the process outlined above becomes the source for a Google AutoML Tables model. To keep things simple, the model was a basic tabular regression. The target variable was the points column, which represents points scored by the home team. I chose this because it was simple, and I was focusing on building a working minimum viable product. The feature set used to predict the target were the dimensions in data frame (field goal percentage, assists, rebounds, etc.). I used root mean squared error (RMSE) as the optimization function, resulting in a RMSE of 4.437 points. The model required about thirty minutes to train and now exists as an artifact in the Google Cloud AI platform that I can update and rerun as I see fit.

Once the model was created and trained in Google AutoML tables, a process similar to the automation explained above for dataset creation was constructed for the generation of predictions resulting from the model. The process includes the following: first, I created Google Cloud Scheduler job that generates a pub/sub message every morning at 9am, one hour after the dataset is updated. Second, a Google Cloud function was written that kicks off when the pub/sub message goes out. This function connects to the Google AutoML tables service and uses the batch predictions function to create predictions and write the result to a BigQuery table. That BigQuery table can then be accessed by anyone who has access and is interested in seeing the predictions generated from the model. Once predictions are generated, that is the end of the entire application workflow.

This paper explains the tools, technologies, and processes that at I employed to construct a cloud application from scratch on Google Cloud Services. This was a learning process for me, with high learning curve as I have not worked in Google Cloud or other cloud environments before I attempted to build this application. However, the app runs, key parts are automated, and it generally follows what I know to be good practice for code source control, management, and delivery. I believe that this project was a great way to learn by doing and am happy with where it landed.


[![Python application test with Gihub Actions](https://github.com/TFM-Devs/MSDS-434-Scaffold/actions/workflows/main.yml/badge.svg)](https://github.com/TFM-Devs/MSDS-434-Scaffold/actions/workflows/main.yml)


